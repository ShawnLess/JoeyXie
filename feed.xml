<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shawnless.github.io/JoeyXie//feed.xml" rel="self" type="application/atom+xml"/><link href="https://shawnless.github.io/JoeyXie//" rel="alternate" type="text/html"/><updated>2025-12-20T08:53:33+00:00</updated><id>https://shawnless.github.io/JoeyXie//feed.xml</id><title type="html">Joey Xie</title><subtitle>Chessground. </subtitle><entry><title type="html">Modern C++ features</title><link href="https://shawnless.github.io/JoeyXie//2025/06/11/Modern-C++-Features.html" rel="alternate" type="text/html" title="Modern C++ features"/><published>2025-06-11T00:00:00+00:00</published><updated>2025-06-11T00:00:00+00:00</updated><id>https://shawnless.github.io/JoeyXie//2025/06/11/Modern-C++-Features</id><content type="html" xml:base="https://shawnless.github.io/JoeyXie//2025/06/11/Modern-C++-Features.html"><![CDATA[<h1 id="coroutine">Coroutine</h1> <p>Coroutine can be seem as “functions whose execution you can pause” and later “resumed”. C++ coroutines are stackless: they suspend execution by returning to the caller, and the data that is required to resume execution is stored separately from the stack.</p> <p><img src="/assets/img/Co-routine.png" alt="Co-Routine" width="70%"/></p> <p>C++ 20 supported with following concepts:</p> <ul> <li><strong>co_await</strong>: to suspend execution until resumed, the arguments for co_await should be <strong>awaitable</strong> instance, which will be explained later.</li> </ul> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="n">task</span><span class="o">&lt;&gt;</span> <span class="n">tcp_echo_server</span><span class="p">()</span>
<span class="p">{</span>
    <span class="kt">char</span> <span class="n">data</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span>
    <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">n</span> <span class="o">=</span> <span class="k">co_await</span>  <span class="n">socket</span><span class="p">.</span><span class="n">async_read_some</span><span class="p">(</span><span class="n">buffer</span><span class="p">(</span><span class="n">data</span><span class="p">));</span>  <span class="c1">//suspend the execution and return to the caller</span>
                        <span class="k">co_await</span>  <span class="n">async_write</span><span class="p">(</span><span class="n">socket</span><span class="p">,</span> <span class="n">buffer</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">));</span>  <span class="c1">//suspend the execution and return to the caller</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure> <ul> <li><strong>co_yeild</strong>: to suspend execution while returning a value:</li> </ul> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="n">generator</span><span class="o">&lt;</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">iota</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span>
        <span class="k">co_yield</span> <span class="n">n</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure> <ul> <li><strong>co_yeild</strong>: complete execution and returning a value</li> </ul> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="n">lazy</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">f</span><span class="p">()</span>
<span class="p">{</span>
    <span class="k">co_return</span> <span class="mi">7</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure> <h2 id="promise-in-coroutine">promise in coroutine</h2> <p>Any function is a coroutine if it contains any of ‘co_wait’, ‘co_yield’ or ‘co_return’. For coroutine contains ‘co_wait’, the return type should alias the special <strong>promise_type</strong> (via <strong>using</strong> or typedef like in general) to the specific class or struct defined by the user. The aliased specific class is generally called <strong>promise</strong>, which must implement some of the specific functions used by the caller to control execution of the coroutine . Coroutine submits its result or exception through this promise object.</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">coroutine_return_obj</span>

<span class="k">struct</span> <span class="nc">my_promise</span>
<span class="p">{</span>
    <span class="c1">//should return object that the coroutine returns</span>
    <span class="n">coroutine_return_obj</span> <span class="n">get_return_object</span><span class="p">()</span> <span class="k">noexcept</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">coroutine_return_obj</span> <span class="p">{</span> <span class="n">std</span><span class="o">::</span><span class="n">coroutine_handle</span><span class="o">&lt;</span><span class="n">my_promise</span><span class="o">&gt;::</span><span class="n">from_promise</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">)</span> <span class="p">};</span>
    <span class="p">};</span>

    <span class="c1">//Called before the co_wait() call.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">suspend_never</span> <span class="n">initial_suspend</span><span class="p">()</span>  <span class="k">const</span> <span class="k">noexcept</span> <span class="p">{</span> <span class="k">return</span> <span class="p">{};</span> <span class="p">}</span>

    <span class="c1">//Called just before the coroutine ends.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">suspend_never</span> <span class="n">final_suspend</span><span class="p">()</span>    <span class="k">const</span> <span class="k">noexcept</span> <span class="p">{</span> <span class="k">return</span> <span class="p">{};</span> <span class="p">}</span>

    <span class="c1">//Called when no co_returned is called at the end.</span>
    <span class="kt">void</span> <span class="n">return_void</span><span class="p">()</span> <span class="k">noexcept</span> <span class="p">{}</span>

     <span class="c1">//Called when some exception happens in coroutine.</span>
    <span class="kt">void</span> <span class="n">unhandled_exception</span><span class="p">()</span> <span class="k">noexcept</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Unhandled exception caught...</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">};</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="C++"/><category term="Boost"/><category term="coroutine"/><summary type="html"><![CDATA[Brief notes of modern C++ features]]></summary></entry><entry><title type="html">Addressing of GPGPU</title><link href="https://shawnless.github.io/JoeyXie//2025/03/24/Addressing-of-GPU-Scaling-Out-Up.html" rel="alternate" type="text/html" title="Addressing of GPGPU"/><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><id>https://shawnless.github.io/JoeyXie//2025/03/24/Addressing-of-GPU-Scaling-Out-Up</id><content type="html" xml:base="https://shawnless.github.io/JoeyXie//2025/03/24/Addressing-of-GPU-Scaling-Out-Up.html"><![CDATA[<h1 id="unified-memory-addressing-uma">Unified Memory Addressing (UMA)</h1> <p>In contemporary computational architecture, the significance of the GPU software ecosystem far outweighs the emphasis placed on hardware performance. Despite the predominant focus of research efforts on enhancing GPU performance metrics, the aspect of programmability remains critically overlooked. Programmability is fundamental to the establishment and evolution of a robust software ecosystem. Specifically, addressing mechanisms in GPUs serve as a pivotal interface for programming practices and memory management strategies. The progression towards greater programming ease has seen the transition from physical addressing to virtual addressing, culminating in the current paradigm of unified memory addressing. This study endeavors to explore both the hardware and software facets inherent to unified addressing, aiming to furnish a deeper understanding of its implications and applications within the realm of GPU architectures.</p> <p>Following code snippet shows how unified memory address simplifies the program:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="code"><pre><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;math.h&gt;</span><span class="cp">
</span> 
<span class="c1">// CUDA kernel to add elements of two arrays</span>
<span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">index</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
 
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span>
  <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">y</span><span class="p">;</span>
 
  <span class="c1">// Allocate Unified Memory -- accessible from CPU or GPU</span>
  <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">x</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
  <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">y</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
 
  <span class="c1">// initialize x and y arrays on the host</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="n">f</span><span class="p">;</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span><span class="n">f</span><span class="p">;</span>
  <span class="p">}</span>
 
  <span class="c1">// Launch kernel on 1M elements on the GPU, NO need to copy data to GPU</span>
  <span class="kt">int</span> <span class="n">blockSize</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">numBlocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">blockSize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">;</span>
  <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span> <span class="c1">//Using the same X/Y pointer</span>
 
  <span class="c1">// Wait for GPU to finish before accessing on host, NO need to copy data from GPU</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
 
  <span class="c1">// Check for errors (all values should be 3.0f)</span>
  <span class="kt">float</span> <span class="n">maxError</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">maxError</span> <span class="o">=</span> <span class="n">fmax</span><span class="p">(</span><span class="n">maxError</span><span class="p">,</span> <span class="n">fabs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mf">3.0</span><span class="n">f</span><span class="p">));</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Max error: "</span> <span class="o">&lt;&lt;</span> <span class="n">maxError</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
 
  <span class="c1">// Free memory</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
 
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Besides simplifying programming, <strong>UMA</strong> also provides following benefits:</p> <ol> <li><strong>Enable Large Data Models</strong>: supports oversubscribe GPU memory Allocate up to system memory size.</li> <li><strong>Simpler Data Access</strong>: CPU/GPU Data coherence Unified memory atomic operations.</li> <li><strong>Performance Turning with prefetching</strong>: Usage hints via <em>cudaMemAdvise</em> API Explicit prefetching API.</li> </ol> <p>The underlying hardware architecture are illustrated in following diagram <a class="citation" href="#allen2021depth">(Allen &amp; Ge, 2021)</a>.</p> <p><img src="/assets/img/UMA-Arch.png" alt="UVM-Arch" width="50%"/></p> <ol> <li> <p>Page Fault propagates to the GPU memory management unit (GMMU), which sends a hardware interrupt to the host. The GMMU writes the corresponding fault information into the GPU Fault Buffer (circular buffer, configured and managed by the UVM driver).</p> </li> <li> <p>GPU sends an interrupt over the interconnect to alert the host UVM driver of a page fault, the host retrieves the complete fault information from the GPU Fault Buffer.</p> </li> <li> <p>Host instructs the GPU to copy pages into its memory via hardware copy engine, and update the page tables.</p> </li> <li> <p>Host instructs the GPU to ‘replay’ the fault, causing uTLB to fetch the page table in GPU DRAM.</p> </li> </ol> <h1 id="addressing-for-scaling">Addressing for Scaling</h1> <p>In contemporary computational environments, the capabilities for Scaling Up and Scaling Out constitute critical features for modern Graphics Processing Units (GPUs). Although NVIDIA has pioneered solutions such as NVLink and NVSwitch to address scalability, there remains a notable absence of an open industry standard for connecting GPUs through ultra-efficient interconnects. Addressing this gap, two emerging initiatives—the <a href="https://www.ualinkconsortium.org/">UALink consortium</a> and the <a href="https://ultraethernet.org/">UltraEthernet Consortium (UEC)</a>—are actively working towards overcoming these scalability challenges. These organizations have meticulously defined the hardware specifications and communication protocols necessary for such advancements. However, their frameworks offer limited discourse on addressing schemes and software programming paradigms. This post aims to propose a potential addressing strategy tailored to mainstream parallel and distributed programming models.</p> <h3 id="parallel-programming-models">Parallel Programming Models</h3> <p>There are two parallel programming models in large distributed system: shared memory (SHMEM) and Message Passing Interface (MPI), as illustrated in follow diagram.</p> <p><img src="/assets/img/Scaling-Programming-Model.png" alt="Scaling Programming Model" width="100%"/></p> <p><strong>SHMEM</strong></p> <ul> <li>Shared data are allocated in <strong>Symmetric Heap</strong>, but each PE manages its memory and the allocated buffer are <strong>different virtual address</strong>. Only size and alignment are coherent between PEs.</li> <li>Memory are accessed using <strong>One Sided</strong> api, which means remote nodes is not aware when and who is access the shared memory.</li> </ul> <p><strong>MPI</strong></p> <ul> <li>No shared memory, only local buffer is used for <strong>temporary</strong> storage.</li> <li>Memory are accessed using <strong>Two Sided</strong> api, which means remote nodes needs to <strong>acknowledge</strong> the transaction.</li> </ul> <p><strong>Fabric API</strong>:</p> <p>Libraries that aim to provide low-level, high-performance communication interfaces for applications in high-performance computing (HPC), cloud, data analytics, and other fields requiring efficient network communication.</p> <ul> <li><strong>UCX</strong>: <a href="https://openucx.org/">https://openucx.org/</a>, unified API that handles many of the complexities of multi-transport environments.</li> <li><strong>Libfabic</strong>: <a href="https://ofiwg.github.io/libfabric/">https://ofiwg.github.io/libfabric/</a>, Fine-grained control over their network operations.</li> </ul> <h3 id="scaling-systems">Scaling Systems</h3> <p>This diagrams shows a general scaling system：</p> <p><img src="/assets/img/Scaling-System.png" alt="Scaling System" width="100%"/></p> <p><strong>Scaling Up</strong> Scaling up means integrating more GPUs in a GPU domain, which shares a single unified memory address space. A typical scaling up system consists of multiple Hosts and GPUs.</p> <ul> <li><strong>GPU Domain</strong>: direct GPU to GPU communication domain, via NVLink/NVSwitch/UALink/UEC etc.</li> <li><strong>Host</strong>: 8X GPU/Host</li> </ul> <p><strong>Scaling Out</strong> Scaling out means more GPU domains connected by high speed network, but with different memory address space.</p> <ul> <li>Host to Host communication, via high speed ethernet fabric or InfiniBand.</li> </ul> <p><strong>Addressing in Scaling Up</strong></p> <p>Recent scaling up system incorporates <strong>unified memory addressing</strong> (e.g., UALink), to facilitate remote memory access, especially small data type accesses (such as word or double word). But this not necessary in distributed programme model.</p> <p>In a shared memory programming model, illustrated in following diagram, we suppose the GPU/GPU are connected with build-in Ethernet Controllers and Ethernet switches.</p> <p><img src="/assets/img/Scaling-Up-Address.png" alt="Scaling Up Address" width="100%"/></p> <p>The scaling up system uses two types of addressing:</p> <ul> <li><strong>System Physical Address</strong>: which is mapped to local GPU physical memories, like HBM.</li> <li><strong>Network Physical Address</strong>: which is mapping to remote GPU. The <strong>NPA</strong> contains the GPU ID that will be used to find the correct MAC address of the destination GPU.</li> </ul> <p><strong>Addressing importing and exporting</strong></p> <p>In scaling up and scaling out system, the GPU under the same OS has is own <strong>private</strong> virtual address space, and not remote access is not allowed. The setup remote memory access, the remote GPU must <strong>export</strong> part of its memory, and other GPU must <strong>import</strong> this memory to its own address space.</p> <p>These <strong>export</strong> and <strong>import</strong> involves multiple soft modules. A <strong>memory handle</strong> is used to pass the information between these modules. For example in HIP programming, <strong>hipIpcMemHandle_t</strong> is defined for these purpose:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre><span class="cp">#define hipIpcMemLazyEnablePeerAccess 0x01
</span>
 <span class="cp">#define HIP_IPC_HANDLE_SIZE 64
</span>
 <span class="c1">// The structure of remote memory handle.</span>
 <span class="k">typedef</span> <span class="k">struct</span> <span class="nc">hipIpcMemHandle_st</span> <span class="p">{</span>
     <span class="kt">char</span> <span class="n">reserved</span><span class="p">[</span><span class="n">HIP_IPC_HANDLE_SIZE</span><span class="p">];</span>
 <span class="p">}</span> <span class="n">hipIpcMemHandle_t</span><span class="p">;</span>

 <span class="c1">//Internal structure of IPC memory handle.</span>
<span class="cp">#define IHIP_IPC_MEM_HANDLE_SIZE   32
</span>
 <span class="k">typedef</span> <span class="k">struct</span> <span class="nc">ihipIpcMemHandle_st</span> <span class="p">{</span>
  <span class="kt">char</span> <span class="n">ipc_handle</span><span class="p">[</span><span class="n">IHIP_IPC_MEM_HANDLE_SIZE</span><span class="p">];</span>  <span class="c1">///&lt; ipc memory handle on ROCr</span>
  <span class="kt">size_t</span> <span class="n">psize</span><span class="p">;</span>
  <span class="kt">size_t</span> <span class="n">poffset</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">owners_process_id</span><span class="p">;</span>
  <span class="kt">char</span> <span class="n">reserved</span><span class="p">[</span><span class="n">IHIP_IPC_MEM_RESERVED_SIZE</span><span class="p">];</span>
<span class="p">}</span> <span class="n">ihipIpcMemHandle_t</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></figure> <ul> <li><strong>export</strong>:</li> </ul> <p>When a remote GPU whens to export a memory region, it calls <strong>hipIpcGetMemHandle()</strong> to get an memory handle, and passed it to other GPUs that wants to import the memory.</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="c1">// export the local device memory, which then be passed to other GPUs for remote access.</span>
<span class="n">hipError_t</span> <span class="nf">hipIpcGetMemHandle</span><span class="p">(</span><span class="n">hipIpcMemHandle_t</span><span class="o">*</span> <span class="n">handle</span><span class="p">,</span> <span class="kt">void</span><span class="o">*</span> <span class="n">devPtr</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></figure> <ul> <li><strong>import</strong>:</li> </ul> <p>The GPU which wants to import the memory calls <strong>hipIpcOpenMemHandle</strong> to import the remote gpu address space:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre> <span class="c1">// Maps memory exported from another process with hipIpcGetMemHandle into the current device address space.</span>
 <span class="c1">// hipIpcMemHandles from each device in a given process may only be opened by one context per device per other process.</span>
<span class="n">hipError_t</span> <span class="nf">hipIpcOpenMemHandle</span><span class="p">(</span><span class="kt">void</span><span class="o">**</span> <span class="n">devPtr</span><span class="p">,</span> <span class="n">hipIpcMemHandle_t</span> <span class="n">handle</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">flags</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></figure> <ul> <li><strong>close</strong>:</li> </ul> <p>After remote GPU used the remote memory, it calls <strong>hipIpcCloseMemHandle</strong> to delete the remote gpu address space from its own address space:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="c1">//close the imported remote memory handle.</span>
<span class="n">hipError_t</span> <span class="nf">hipIpcCloseMemHandle</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span> <span class="n">devPtr</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><img src="/assets/img/Scaling-Import-Export.png" alt="Scaling Import and Export " width="70%"/></p> <p>During these import/export and the following remote memory process, these MMUs are involved: Three MMUs are used during GPU to GPU communication:</p> <ul> <li><strong>GPU MMU</strong>: setup the TLB table during import, and translate translate virtual address to NPA.</li> <li><strong>Port MMU</strong>: A table in Ethernet controller, or just a software implementation which maps GPU-ID to network MAC address.</li> <li><strong>R-MMU</strong>:Target GPU MMU translate NPA to local SPA, also do accessible control and checks.</li> </ul> <p><strong>Addressing mapping in Shared Programming Model</strong></p> <p>In shared memory programming model, the MMU setup and translation process can be described as Following:</p> <p><img src="/assets/img/Scaling-Shared-Mem.png" alt="Scaling-Up-Shared-Mem" width="100%"/></p> <ol> <li> <p><strong>shmem_init()</strong>: The Shared Memory library will build up a <strong>segment table</strong> first, which will record the <strong>shared</strong> memory segments, including the start address and size. This table is <strong>shared</strong> between all PEs and will be referenced when accessed remotely.</p> </li> <li><strong>shmem_malloc()</strong>: User programming applies memories in the Symmetric shared heap. <ul> <li>All PE will do the same <strong>malloc()</strong> action and the <strong>shmem_malloc()</strong> will only return after all PE completes its operation.</li> <li>shmem_malloc() returns <strong>Local Virtual Address</strong> and different PE returns <strong>different VA</strong>, this VA only valid in this PE.</li> <li>The library will <strong>register</strong> the allocated memory regions in *R-MMU<strong>, so remote access are allowed for this memory region, and **PIN</strong> the corresponding physical pages so OS won’t swap out the pages.</li> </ul> </li> <li><strong>shmem_access()</strong>: User program access remote memory via <strong>Local Virtual Address</strong> returned by malloc() and destination PE ID: <ul> <li><strong>Symmetric Offset</strong> are calculated using <strong>segment table</strong> and <strong>local VA</strong>.</li> <li><strong>Remote VA</strong> are generated using <strong>segment table</strong> and <strong>Symmetric Offset</strong>.</li> <li><strong>Network Packet</strong> are composed using <strong>remote VA</strong>, <strong>data</strong> and <strong>command</strong>.</li> <li><strong>Remote PE</strong>: Remote PE parsed the network packet, extract the <strong>remote VA</strong> as its <strong>local VA</strong>, and do the accesses. <strong>local VA</strong> are then translated to physical address with <strong>R-MMU</strong>.</li> </ul> </li> </ol> <hr/> <h3 id="reference">Reference</h3> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div id="allen2021depth" class="col-sm-10"> <div class="title">In-depth analyses of unified virtual memory system for GPU accelerated computing</div> <div class="author"> Tyler Allen,&nbsp;and&nbsp;Rong Ge </div> <div class="periodical"> <em>In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol>]]></content><author><name></name></author><category term="GPU"/><category term="Scaling-Up"/><category term="Scaling-Out"/><summary type="html"><![CDATA[Addressing scheme of GPU.]]></summary></entry><entry><title type="html">Discriminative learning vs Generative learning</title><link href="https://shawnless.github.io/JoeyXie//2025/02/21/Discriminative-Generative.html" rel="alternate" type="text/html" title="Discriminative learning vs Generative learning"/><published>2025-02-21T00:00:00+00:00</published><updated>2025-02-21T00:00:00+00:00</updated><id>https://shawnless.github.io/JoeyXie//2025/02/21/Discriminative-Generative</id><content type="html" xml:base="https://shawnless.github.io/JoeyXie//2025/02/21/Discriminative-Generative.html"><![CDATA[<p>Taking logistic regression as an example, the <strong>average empirical loss function</strong> is:</p> \[J(\theta)= - \frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})+(1-y^{(i)})log ((1-h(x^{(i)}))\] <p>where $ y^{(i)} \in {0,1}$, $h_\theta(x)=g(\theta^Tx), g(z)=1/(1-e^{-z}), x^{(i)}={x_0, x_1, …, x_n} $.</p> <p>for <strong>discriminative learning</strong>, we can update the $\theta$ with Newton’s method.</p> \[\frac{\delta J(\theta)}{\theta_j}= -\frac{\delta}{\theta_j}\{\frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(g(\theta^T x^{(i)})+(1-y^{(i)})log ((1-g(\theta^Tx^{(i)})) \}\] \[=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)} \frac{1}{(g(\theta^T x^{(i)})}g'(\theta^T x^{(i)})x^{(i)}_j + (1-y^{(i)}) \frac{-1} {((1-g(\theta^Tx^{(i)}))} g'(\theta^T x^{(i)})x^{(i)}_j\] <p>Given $ g’(z) = g(z)(1 - g(z)) $, the above equation become as:</p> \[= - \frac{1}{m}\sum_{i=1}^{m}y^{(i)} \frac{1}{(g(\theta^T x^{(i)})}g(\theta^T x^{(i)})(1-g(\theta^T x^{(i)}))x^{(i)}_j - (1-y^{(i)}) \frac{1} {((1-g(\theta^Tx^{(i)}))} g(\theta^T x^{(i)})(1-g(\theta^T x^{(i)}))x^{(i)}_j\] \[= - \frac{1}{m}\sum_{i=1}^{m}y^{(i)} (1-g(\theta^T x^{(i)}))x^{(i)}_j - (1-y^{(i)}) g(\theta^T x^{(i)})x^{(i)}_j\] \[= - \frac{1}{m}\sum_{i=1}^{m}y^{(i)}x^{(i)}_j -y^{(i)}g(\theta^T x^{(i)})x^{(i)}_j - g(\theta^T x^{(i)})x^{(i)}_j + y^{(i)}) g(\theta^T x^{(i)})x^{(i)}_j\] \[= \frac{1}{m}\sum_{i=1}^{m}( g(\theta^T x^{(i)}) - y^{(i)} )x^{(i)}_j\] <p>Written in matrix form, where X’s dimension is <strong>m..n</strong>:</p> \[\triangledown_\theta J(\theta) = \frac{1}{m} X ^T (g(X\theta) - Y)\] <p>We then get the <strong>Hessian</strong> matrix:</p> \[H_{jk} = \frac{\delta ( \frac {\delta J(\theta)} {\theta_j} )} {\theta_k} = \frac{\delta ( \frac{1}{m}\sum_{i=1}^{m}( g(\theta^T x^{(i)}) - y^{(i)} )x^{(i)}_j )} {\theta_k}\] \[= \frac{1}{m}\sum_{i=1}^{m}g'(\theta^T x^{(i)})x^{(i)}_j x^{(i)}_k\] <p>Remember $ g’(z) = g(z)(1 - g(z)) $, we get:</p> \[H_{jk} = \frac{1}{m}\sum_{i=1}^{m}g(\theta^T x^{(i)}) (1 - g(\theta^T x^{(i)}) )x^{(i)}_j x^{(i)}_k\] <p>If we define $ d^{(i)}=g(\theta^Tx^{(i)}) (1 - g(\theta^Tx^{(i)}))$, and D={ $d^{(1)}, d^{(2)}, … , d^{(m)} $}</p> <p>then we get:</p> \[H = \begin{bmatrix} &amp;... &amp;... &amp;... &amp;...\\ &amp;x^{(1)}_jd^{(1)} &amp;x^{(2)}_jd^{(2)} &amp;x^{(...)}_jd^{(...)} &amp;x^{(m)}_jd^{(m)} \\ &amp;... &amp;... &amp;... &amp;... \\ \end{bmatrix} \begin{bmatrix} &amp;... &amp;x^{(1)}_k &amp;... \\ &amp;... &amp;x^{(2)}_k &amp;... \\ &amp;... &amp;x^{(...)}_k &amp;... \\ &amp;... &amp;x^{(m)}_k &amp;... \\ \end{bmatrix}\] \[= (X \bullet D)^TX\] <hr/>]]></content><author><name></name></author><category term="Math"/><category term="ML"/><summary type="html"><![CDATA[A concrete comparison of the discriminative and generative learning.]]></summary></entry><entry><title type="html">Fundamental Math for Machine Learning</title><link href="https://shawnless.github.io/JoeyXie//2025/01/27/Fundamental-Math-for-Machine-Learning.html" rel="alternate" type="text/html" title="Fundamental Math for Machine Learning"/><published>2025-01-27T00:00:00+00:00</published><updated>2025-01-27T00:00:00+00:00</updated><id>https://shawnless.github.io/JoeyXie//2025/01/27/Fundamental-Math-for-Machine-Learning</id><content type="html" xml:base="https://shawnless.github.io/JoeyXie//2025/01/27/Fundamental-Math-for-Machine-Learning.html"><![CDATA[<h2 id="linear-regression">Linear Regression</h2> <p>Linear Regression is an algorithm which predicts unknown value with existing data set. It models the factors and results as linear function, for example:</p> \[h_\theta (x)=\theta_0 + \theta_1x1 + \theta_2x2\] <p>where $x_1,x_2$ are the <strong>factors</strong> which affect the result $h_\theta (x)$. $x_1,x_2$ are also called <strong>features</strong> in deep learning. $\theta_0, \theta_1,\theta_2$ are the parameters or <strong>weights</strong> in deep learning, which are supposed to be fixed during the prediction or <strong>inference</strong>.</p> <p>If we let $x_0$=1, then we can write the above equation in a more general form as</p> \[h_\theta (x) =\sum_{i=1}^{n}\theta_ix_i=\mathbf{\theta}^T\mathbf{x}\] <p>where</p> \[\mathbf{\theta} = \begin{bmatrix} \theta_0 \\ \theta_1 \\ ... \\ \theta_n \\ \end{bmatrix} , \mathbf{x} = \begin{bmatrix} x_0 \\ x_1 \\ ... \\ x_n \\ \end{bmatrix}\] <p>Given an concrete example: we suppose the house price is highly related to 1. area, and 2. number of bedroom, how can we predict the price given its area and number of bedrooms?</p> <table> <thead> <tr> <th>Living area ($ft^2$)</th> <th>#bedrooms</th> <th>price (k)</th> </tr> </thead> <tbody> <tr> <td>2104</td> <td>3</td> <td>400</td> </tr> <tr> <td>1600</td> <td>3</td> <td>330</td> </tr> <tr> <td>2400</td> <td>3</td> <td>369</td> </tr> <tr> <td>1416</td> <td>2</td> <td>232</td> </tr> <tr> <td>3000</td> <td>4</td> <td>540</td> </tr> <tr> <td>…</td> <td>…</td> <td>…</td> </tr> </tbody> </table> <p>In this example: $x_1$ is the living area, $x_2$ is the number of bedrooms, <strong>y</strong> is the price of the house.</p> <p>The table about is called a <strong>training set</strong>, which will be used to training our model (that is the value of $\theta$s).</p> <p>The straight thought is to choose the hypnosis <strong>h(x)</strong> close to the training data <strong>y</strong>.</p> <p>This can be expressed with <strong>cost function</strong>. There can be many types of cost functions, the most popular one is least-squares cost function:</p> \[J(\theta) = \frac{1}{2}\sum_{i=1}^{n} (h_\theta(x^i) - y^i)^2\] <h3 id="gradient-descent-algorithm">Gradient Descent Algorithm</h3> <p>Gradient Descent Algorithm is used to find the value of $\theta$ so that the $J(\theta)$ is minimized, which can be described as following:</p> \[\theta_j := \theta_j - \alpha \frac{\delta J(\theta ) }{\delta \theta_j}\] <p>Here $\delta$ is called <strong>learning rate</strong>.</p> <p>Expend the $J(\theta)$ we can get the update equation:</p> \[\theta_j := \theta_j + \alpha \sum_{i=1}^{m} (y^i - h_\theta(x^i))x_j^i\] <p>where <strong>i</strong> is the index of data sets, <strong>j</strong> is the index of <strong>features</strong></p> <h4 id="batch-gradient-descent">batch gradient descent</h4> <p>In the above method, we look at every example in the entire training set on every step, and is called <strong>batch gradient descent</strong></p> <p>repeat until converge {</p> \[\theta_0 := \theta_0 + \alpha \sum_{i=1}^{m} (y^i - h_\theta(x^i))x_0^i\] \[\theta_1 := \theta_1 + \alpha \sum_{i=1}^{m} (y^i - h_\theta(x^i))x_i^i\] \[...\] \[\theta_n := \theta_n + \alpha \sum_{i=1}^{m} (y^i - h_\theta(x^i))x_n^i\] <p>}</p> <ul> <li>Pros: update of $\theta$s point to the deepest slope, which converges more quickly (TODO: illustrate with diagram)</li> <li>Cons: need to transverse the whole data set (1..m) in each step, not efficient for large training data.</li> </ul> <h4 id="stochastic-gradient-descent">stochastic gradient descent</h4> <p>If we only update the $\theta$ with current data set’s gradient error, the algorithm runs more efficiently:</p> <p>repeat until meet the goal {</p> <p>for i= 1…m {</p> \[\theta_0 := \theta_0 + \alpha (y^i - h_\theta(x^i))x_0^i\] \[\theta_1 := \theta_1 + \alpha (y^i - h_\theta(x^i))x_i^i\] \[...\] \[\theta_n := \theta_n + \alpha (y^i - h_\theta(x^i))x_n^i\] <p>}</p> <p>}</p> <ul> <li>Pros: Efficient,especially for large training set.</li> <li>Cons: may never converge, but should be close to the target.</li> </ul> <h4 id="the-normal-equations">The normal equations</h4> <p>For linear regression, there is a more efficient way to get the value of $\theta$, which computed directly from matrix. We omit the deduction process and only give the results here.</p> <p>Let matrix <strong>X</strong>(m..n) represents all of the x values in the training set, and vector $\overrightarrow{y}$ (n..1)</p> \[\mathbf{X} = \begin{bmatrix}x_0^0, x_1^0, ..., x_n^0 \\ x_0^1, x_1^1, ..., x_n^1 \\ ... \\ x_0^m, x_1^m, ..., x_n^m \end{bmatrix} , \mathbf{Y} = \begin{bmatrix} x_0 \\ x_1 \\ ... \\ x_m \end{bmatrix}\] <p>Then the value of $\theta$ that minimizes $J(\theta)$ is given in closed form by the equation:</p> \[\theta = (X^TX)^{-1}X^T \overrightarrow{y}\] <p>This is called the <strong>normal equation</strong>.</p> <h4 id="probabilistic-interpretation-of-the-cost-function">Probabilistic interpretation of the cost function</h4> <p>We defined the cost function as:</p> \[J(\theta) = \frac{1}{2}\sum_{i=1}^{n} (h_\theta(x^i) - y^i)^2\] <p>Why we chose this? is it optimal? This can be deducted from the likelihood.</p> <p>We can model the targe value y is a distribution of x:</p> \[y^{(i)} = \theta ^T x^{(i)} + \epsilon ^ {(i)}\] <p>Where $ \epsilon ^ {(i)} $ models the unknown factor that might affect the result. <strong>We assume</strong> $ \epsilon ^ {(i)} $ <strong>Normal distribution</strong> with mean 0 and variance \sigma. Then each sample of $y^{(i)}$ is a <strong>conditional probability</strong> with normal distribution whose mean value is $\theta ^T x^{(i)} $:</p> \[p(y^{(i)} | x^{(i)} ; \theta) = \frac{1}{\sqrt{2\pi\sigma}} exp (-\frac{ (y^{(i)} - \theta ^T x^{(i)}) ^2 } {2\sigma^2})\] <p>Written in matrix form:</p> <p>given <strong>X</strong> and $\theta$, the conditional probability of $\overrightarrow{y}$ can be write as $p(\overrightarrow{y} | X; \theta ）$, where $\theta$ are fixed values.</p> <p>We further assume $ \epsilon ^ {(i)} $ are independent to each other, which means the distribution of $ y ^ {(i)} $ are independent to each other. Then the <strong>likelihood</strong> function of y can be expressed as:</p> \[L(\theta) = p(\overrightarrow{y} | X; \theta ）= \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi\sigma}} exp (-\frac{ (y^{(i)} - \theta ^T x^{(i)}) ^2 } {2\sigma^2})\] <p>The logic is given $x^{(1)},x^{(2)} … x^{(m)}$ , the probability of $y^{(1)},y^{(2)} … y^{(m)}$ (which are the probability that all of the <strong>y</strong>s occurs at the same time with the exactly value) are</p> \[p(y^{(1)} | x^{(1)}) * p(y^{(2)} | x^{(2)}) * ... * p(y^{(m)} | x^{(m)})\] <p>Given all that assumption, what might be the best values of $\theta$? in <strong>Maximum Likelihood</strong> theory, we should choose the $\theta$ so that the probability of observed data set (that is our training set) should be maximized.</p> <p>Then the question turns out to be finding the maximum value of $L(\theta)$. To facilitate the deduction, we can also maximum $log(L(\theta))$. If we substitute $L(\theta)$, we can get the result that the optimal value of $\theta$ are those minimizes:</p> \[\frac{1}{2}\sum_{i=1}^{n} (h_\theta(x^i) - y^i)^2\] <p>which are the same as <strong>least square</strong> cost function.</p> <h2 id="logistic-regression">Logistic Regression</h2> <p>logistic regression only outputs 0 or 1, e.g., to decide if an email is spam or not. We define a different hypotheses for this prediction, which is called <strong>sigmod function</strong></p> \[h_\theta(x)=\frac{1}{1-e^{-\theta^Tx}}\] <p>Using the <strong>maximum likelihood</strong> methodology, we can get update rule for stochastic descent as:</p> \[\theta_j := \theta_j + \alpha \sum_{i=1}^{m} (y^i - h_\theta(x^i))x_j^i\] <p>which is the same as in linear regression except the $h_\theta(x)$ is different.</p> <h2 id="newtons-method">Newton’s method</h2> <p>Newton’s method is another way to find the optimal $\theta$, which can be given as:</p> \[\theta := \theta - H^{-1}\triangledown_\theta l (\theta), \triangledown_\theta l (\theta)=\begin{bmatrix} \frac {\delta l (\theta)} {\theta_1} \\ \frac {\delta l (\theta)} {\theta_2} \\ ... \\ \frac {\delta l (\theta)} {\theta_n} \end{bmatrix}\] <p>And <strong>H</strong> is callded <strong>Hessian matrix</strong>, whose element is:</p> \[H_{ij}=\frac{\delta ^2 l (\theta)} {\theta_i \theta_j}\] <h2 id="generalized-linear-models">Generalized Linear Models</h2> <p>Most of the linear models can be expressed in a more generalized form:</p> \[p(y|\eta )= b(y)e^{\eta^TT(y)-a(\eta)}\] <table> <thead> <tr> <th>parameters</th> <th>linear regression</th> <th>logistic regression</th> <th>Softmax Regression</th> </tr> </thead> <tbody> <tr> <td>$h_\theta(x)$</td> <td>$ \theta^Tx $</td> <td>$ \frac{1}{1+e^{-\theta^Tx}}$</td> <td>$\frac{e^{\theta_i^Tx}} {\sum_{i=1}^{k}e^{\theta_j^Tx} }$</td> </tr> <tr> <td>$\eta$</td> <td>$\mu$</td> <td>$ log \frac{\varphi }{1- \varphi}$</td> <td>$ log \frac{\varphi_i }{ \varphi_k}$</td> </tr> <tr> <td>b(y)</td> <td>$\frac{1}{\sqrt{2\pi\sigma}} exp (-\frac{ y^2}{2})$</td> <td>1</td> <td>1</td> </tr> <tr> <td>T(y)</td> <td>y</td> <td>y</td> <td>a matrix mapping</td> </tr> <tr> <td>$a(\eta)$</td> <td>$\frac {\eta^2} {2}$</td> <td>$log( \frac{1}{1+e^\eta})$</td> <td>$-log(\varphi_k) $</td> </tr> </tbody> </table> <h2 id="generative-learning">Generative Learning</h2> <ol> <li><strong>Discriminative learning</strong>: learn <strong>p(y|x)</strong> directly from the data set {<strong>X</strong>, $\overrightarrow{y}$}.</li> <li><strong>Generative learning</strong>: model <strong>p(x|y)</strong> and <strong>p(y)</strong>, and then model <strong>p(y|x)</strong>.</li> </ol> <p>For example, if we trying to classify the pictures between monkey and dogs:</p> \[\begin{cases} &amp; y=1, \text{is a monkey}\\ &amp; y=0, \text{is a dog} \end{cases}\] <p>For discriminative learning, it will model the conditional distribution of y given x, and will find a straight line in the space <strong>x</strong>, and then classify the a new animal as either as monkey or dog.</p> <p>For generative learning, it first training a model of <strong>p(x|y=1)</strong>, which means the distribution of x when y=1 (monkey); and then training a model of <strong>p(x|y=0)</strong>, which means the distribution of x when y=0 (dog). For new pictures or new input x, we fit it into <strong>p(x|y=1)</strong> and <strong>p(x|y=0)</strong>, to see which model fits best and make the decisions.</p> <p>After modeling <strong>p(x|y)</strong> and <strong>p(y)</strong>, we can then get <strong>p(x|y)</strong> using <strong>Bayes rule</strong>:</p> \[p(y|x_1,x_2 ... x_n) = \frac {p(x_1,x_2,...x_n | y) p(y)} {p(x_1, x_2, ... x_3)} = \frac {p(x_1,x_2,...x_n | y) p(y)} {p(x_1, x_2, ... x_3|y=1)p(y=1) + p(x_1, x_2, ... x_3|y=0)p(y=0) }\] <h3 id="gaussian-discriminant-analysis">Gaussian discriminant analysis</h3> <p>In Gaussian discriminant analysis (GDA), we assume that p(x|y) is distributed according to a multivariate normal distribution:</p> \[p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-u)))\] <p>where $\mu \in \mathbb{R}^n$ is the <strong>mean vector</strong> of <strong>x</strong>, and $\Sigma \in \mathbb{R}^{n \times n}$ is the covariance of <strong>x</strong>. The multivariate normal distribution can also written as “$N(\mu, \Sigma)$”.</p> <p>Back to the binary classification problem. In generative learning we model the distribution:</p> \[\begin{align} p(y) &amp;= \phi ^y (1-\phi)^y \\ p(x|y=0) &amp;= N(\mu_0,\Sigma) \\ p(x|y=1) &amp;= N(\mu_1,\Sigma) \end{align}\] <p>The <strong>p(y)</strong> is called the <strong>prior probability</strong> which is what we assumed and can be evaluated and sampled with the training set (we also can evaluate the <strong>p(y=0)</strong> and <strong>p(y=1)</strong> with the training set). Given evaluated $\mu, \Sigma$, <strong>p(x|y=0)</strong> and <strong>p(x|y=1)</strong> are the known distribution, and we then can calculate with bayes rule:</p> \[p(y|x)= \frac {p(x|y) p(y)}{ p(x|y=0)p(y=0) + p(x|y=1)p(y=1)}\] <p>For train set $x={ x^{(0)} … x^{(m)}}$ and $y={ y^{(0)} … y^{(m)}}$, we can evaluate the parameter by maximized the log-likelihood function:</p> \[\begin{align} \phi &amp;= \frac{1}{m} \sum_{i=1}^{m} 1\{y^{(i)} =1 \} \\ \mu_0 &amp;= \frac{ \sum_{i=1}^{m} 1\{y^{(i)} =0 \}x^{(i)}} {\sum_{i=1}^{m} 1\{y^{(i)} =0 \}} \\ \mu_1 &amp;= \frac{ \sum_{i=1}^{m} 1\{y^{(i)} =1 \}x^{(i)}} {\sum_{i=1}^{m} 1\{y^{(i)} =1 \}} \\ \Sigma &amp;= \frac{1}{m} \sum_{i=1}^{m} (x^{(i)}- \mu_{y^{(i)}} )) (x^{(i)}- \mu_{y^{(i)}} )^T \\ \end{align}\] <p>For more detailed coding and result illustration, please refer to <a href="https://shawnless.github.io/JoeyXie//2025/02/21/Discriminative-Generative.html">discriminative vs generative</a>.</p> <hr/>]]></content><author><name></name></author><category term="Math"/><category term="ML"/><summary type="html"><![CDATA[A learning notes of the basic math knowledge and conceptions for machine learning.]]></summary></entry></feed>