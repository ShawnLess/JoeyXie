<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Addressing of GPGPU | </title> <meta name="author" content=" "> <meta name="description" content="Addressing scheme of GPU."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shawnless.github.io/JoeyXie//2025/03/24/Addressing-of-GPU-Scaling-Out-Up.html"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"><strong>Joey </strong> Xie </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> <li class="nav-link"> English <a href="/zh_CN/2025/03/24/Addressing-of-GPU-Scaling-Out-Up.html"> 中文 </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Addressing of GPGPU</h1> <p class="post-meta"> Created in March 24, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> GPU   <i class="fa-solid fa-hashtag fa-sm"></i> Scaling-Up   <i class="fa-solid fa-hashtag fa-sm"></i> Scaling-Out </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="unified-memory-addressing-uma">Unified Memory Addressing (UMA)</h1> <p>In contemporary computational architecture, the significance of the GPU software ecosystem far outweighs the emphasis placed on hardware performance. Despite the predominant focus of research efforts on enhancing GPU performance metrics, the aspect of programmability remains critically overlooked. Programmability is fundamental to the establishment and evolution of a robust software ecosystem. Specifically, addressing mechanisms in GPUs serve as a pivotal interface for programming practices and memory management strategies. The progression towards greater programming ease has seen the transition from physical addressing to virtual addressing, culminating in the current paradigm of unified memory addressing. This study endeavors to explore both the hardware and software facets inherent to unified addressing, aiming to furnish a deeper understanding of its implications and applications within the realm of GPU architectures.</p> <p>Following code snippet shows how unified memory address simplifies the program:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td> <td class="code"><pre><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;math.h&gt;</span><span class="cp">
</span> 
<span class="c1">// CUDA kernel to add elements of two arrays</span>
<span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">index</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
 
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span>
  <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">y</span><span class="p">;</span>
 
  <span class="c1">// Allocate Unified Memory -- accessible from CPU or GPU</span>
  <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">x</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
  <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">y</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
 
  <span class="c1">// initialize x and y arrays on the host</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="n">f</span><span class="p">;</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span><span class="n">f</span><span class="p">;</span>
  <span class="p">}</span>
 
  <span class="c1">// Launch kernel on 1M elements on the GPU, NO need to copy data to GPU</span>
  <span class="kt">int</span> <span class="n">blockSize</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">numBlocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">blockSize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">;</span>
  <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span> <span class="c1">//Using the same X/Y pointer</span>
 
  <span class="c1">// Wait for GPU to finish before accessing on host, NO need to copy data from GPU</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
 
  <span class="c1">// Check for errors (all values should be 3.0f)</span>
  <span class="kt">float</span> <span class="n">maxError</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">maxError</span> <span class="o">=</span> <span class="n">fmax</span><span class="p">(</span><span class="n">maxError</span><span class="p">,</span> <span class="n">fabs</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mf">3.0</span><span class="n">f</span><span class="p">));</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Max error: "</span> <span class="o">&lt;&lt;</span> <span class="n">maxError</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
 
  <span class="c1">// Free memory</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
  <span class="n">cudaFree</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
 
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Besides simplifying programming, <strong>UMA</strong> also provides following benefits:</p> <ol> <li> <strong>Enable Large Data Models</strong>: supports oversubscribe GPU memory Allocate up to system memory size.</li> <li> <strong>Simpler Data Access</strong>: CPU/GPU Data coherence Unified memory atomic operations.</li> <li> <strong>Performance Turning with prefetching</strong>: Usage hints via <em>cudaMemAdvise</em> API Explicit prefetching API.</li> </ol> <p>The underlying hardware architecture are illustrated in following diagram <a class="citation" href="#allen2021depth">(Allen &amp; Ge, 2021)</a>.</p> <p><img src="/assets/img/UMA-Arch.png" alt="UVM-Arch" width="50%"></p> <ol> <li> <p>Page Fault propagates to the GPU memory management unit (GMMU), which sends a hardware interrupt to the host. The GMMU writes the corresponding fault information into the GPU Fault Buffer (circular buffer, configured and managed by the UVM driver).</p> </li> <li> <p>GPU sends an interrupt over the interconnect to alert the host UVM driver of a page fault, the host retrieves the complete fault information from the GPU Fault Buffer.</p> </li> <li> <p>Host instructs the GPU to copy pages into its memory via hardware copy engine, and update the page tables.</p> </li> <li> <p>Host instructs the GPU to ‘replay’ the fault, causing uTLB to fetch the page table in GPU DRAM.</p> </li> </ol> <h1 id="addressing-for-scaling">Addressing for Scaling</h1> <p>In contemporary computational environments, the capabilities for Scaling Up and Scaling Out constitute critical features for modern Graphics Processing Units (GPUs). Although NVIDIA has pioneered solutions such as NVLink and NVSwitch to address scalability, there remains a notable absence of an open industry standard for connecting GPUs through ultra-efficient interconnects. Addressing this gap, two emerging initiatives—the <a href="https://www.ualinkconsortium.org/" rel="external nofollow noopener" target="_blank">UALink consortium</a> and the <a href="https://ultraethernet.org/" rel="external nofollow noopener" target="_blank">UltraEthernet Consortium (UEC)</a>—are actively working towards overcoming these scalability challenges. These organizations have meticulously defined the hardware specifications and communication protocols necessary for such advancements. However, their frameworks offer limited discourse on addressing schemes and software programming paradigms. This post aims to propose a potential addressing strategy tailored to mainstream parallel and distributed programming models.</p> <h3 id="parallel-programming-models">Parallel Programming Models</h3> <p>There are two parallel programming models in large distributed system: shared memory (SHMEM) and Message Passing Interface (MPI), as illustrated in follow diagram.</p> <p><img src="/assets/img/Scaling-Programming-Model.png" alt="Scaling Programming Model" width="100%"></p> <p><strong>SHMEM</strong></p> <ul> <li>Shared data are allocated in <strong>Symmetric Heap</strong>, but each PE manages its memory and the allocated buffer are <strong>different virtual address</strong>. Only size and alignment are coherent between PEs.</li> <li>Memory are accessed using <strong>One Sided</strong> api, which means remote nodes is not aware when and who is access the shared memory.</li> </ul> <p><strong>MPI</strong></p> <ul> <li>No shared memory, only local buffer is used for <strong>temporary</strong> storage.</li> <li>Memory are accessed using <strong>Two Sided</strong> api, which means remote nodes needs to <strong>acknowledge</strong> the transaction.</li> </ul> <p><strong>Fabric API</strong>:</p> <p>Libraries that aim to provide low-level, high-performance communication interfaces for applications in high-performance computing (HPC), cloud, data analytics, and other fields requiring efficient network communication.</p> <ul> <li> <strong>UCX</strong>: <a href="https://openucx.org/" rel="external nofollow noopener" target="_blank">https://openucx.org/</a>, unified API that handles many of the complexities of multi-transport environments.</li> <li> <strong>Libfabic</strong>: <a href="https://ofiwg.github.io/libfabric/" rel="external nofollow noopener" target="_blank">https://ofiwg.github.io/libfabric/</a>, Fine-grained control over their network operations.</li> </ul> <h3 id="scaling-systems">Scaling Systems</h3> <p>This diagrams shows a general scaling system：</p> <p><img src="/assets/img/Scaling-System.png" alt="Scaling System" width="100%"></p> <p><strong>Scaling Up</strong> Scaling up means integrating more GPUs in a GPU domain, which shares a single unified memory address space. A typical scaling up system consists of multiple Hosts and GPUs.</p> <ul> <li> <strong>GPU Domain</strong>: direct GPU to GPU communication domain, via NVLink/NVSwitch/UALink/UEC etc.</li> <li> <strong>Host</strong>: 8X GPU/Host</li> </ul> <p><strong>Scaling Out</strong> Scaling out means more GPU domains connected by high speed network, but with different memory address space.</p> <ul> <li>Host to Host communication, via high speed ethernet fabric or InfiniBand.</li> </ul> <p><strong>Addressing in Scaling Up</strong></p> <p>Recent scaling up system incorporates <strong>unified memory addressing</strong> (e.g., UALink), to facilitate remote memory access, especially small data type accesses (such as word or double word). But this not necessary in distributed programme model.</p> <p>In a shared memory programming model, illustrated in following diagram, we suppose the GPU/GPU are connected with build-in Ethernet Controllers and Ethernet switches.</p> <p><img src="/assets/img/Scaling-Up-Address.png" alt="Scaling Up Address" width="100%"></p> <p>The scaling up system uses two types of addressing:</p> <ul> <li> <strong>System Physical Address</strong>: which is mapped to local GPU physical memories, like HBM.</li> <li> <strong>Network Physical Address</strong>: which is mapping to remote GPU. The <strong>NPA</strong> contains the GPU ID that will be used to find the correct MAC address of the destination GPU.</li> </ul> <p><strong>Addressing importing and exporting</strong></p> <p>In scaling up and scaling out system, the GPU under the same OS has is own <strong>private</strong> virtual address space, and not remote access is not allowed. The setup remote memory access, the remote GPU must <strong>export</strong> part of its memory, and other GPU must <strong>import</strong> this memory to its own address space.</p> <p>These <strong>export</strong> and <strong>import</strong> involves multiple soft modules. A <strong>memory handle</strong> is used to pass the information between these modules. For example in HIP programming, <strong>hipIpcMemHandle_t</strong> is defined for these purpose:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td> <td class="code"><pre><span class="cp">#define hipIpcMemLazyEnablePeerAccess 0x01
</span>
 <span class="cp">#define HIP_IPC_HANDLE_SIZE 64
</span>
 <span class="c1">// The structure of remote memory handle.</span>
 <span class="k">typedef</span> <span class="k">struct</span> <span class="nc">hipIpcMemHandle_st</span> <span class="p">{</span>
     <span class="kt">char</span> <span class="n">reserved</span><span class="p">[</span><span class="n">HIP_IPC_HANDLE_SIZE</span><span class="p">];</span>
 <span class="p">}</span> <span class="n">hipIpcMemHandle_t</span><span class="p">;</span>

 <span class="c1">//Internal structure of IPC memory handle.</span>
<span class="cp">#define IHIP_IPC_MEM_HANDLE_SIZE   32
</span>
 <span class="k">typedef</span> <span class="k">struct</span> <span class="nc">ihipIpcMemHandle_st</span> <span class="p">{</span>
  <span class="kt">char</span> <span class="n">ipc_handle</span><span class="p">[</span><span class="n">IHIP_IPC_MEM_HANDLE_SIZE</span><span class="p">];</span>  <span class="c1">///&lt; ipc memory handle on ROCr</span>
  <span class="kt">size_t</span> <span class="n">psize</span><span class="p">;</span>
  <span class="kt">size_t</span> <span class="n">poffset</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">owners_process_id</span><span class="p">;</span>
  <span class="kt">char</span> <span class="n">reserved</span><span class="p">[</span><span class="n">IHIP_IPC_MEM_RESERVED_SIZE</span><span class="p">];</span>
<span class="p">}</span> <span class="n">ihipIpcMemHandle_t</span><span class="p">;</span>
</pre></td> </tr></tbody></table></code></pre></figure> <ul> <li> <strong>export</strong>:</li> </ul> <p>When a remote GPU whens to export a memory region, it calls <strong>hipIpcGetMemHandle()</strong> to get an memory handle, and passed it to other GPUs that wants to import the memory.</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="c1">// export the local device memory, which then be passed to other GPUs for remote access.</span>
<span class="n">hipError_t</span> <span class="nf">hipIpcGetMemHandle</span><span class="p">(</span><span class="n">hipIpcMemHandle_t</span><span class="o">*</span> <span class="n">handle</span><span class="p">,</span> <span class="kt">void</span><span class="o">*</span> <span class="n">devPtr</span><span class="p">);</span>
</pre></td> </tr></tbody></table></code></pre></figure> <ul> <li> <strong>import</strong>:</li> </ul> <p>The GPU which wants to import the memory calls <strong>hipIpcOpenMemHandle</strong> to import the remote gpu address space:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
</pre></td> <td class="code"><pre> <span class="c1">// Maps memory exported from another process with hipIpcGetMemHandle into the current device address space.</span>
 <span class="c1">// hipIpcMemHandles from each device in a given process may only be opened by one context per device per other process.</span>
<span class="n">hipError_t</span> <span class="nf">hipIpcOpenMemHandle</span><span class="p">(</span><span class="kt">void</span><span class="o">**</span> <span class="n">devPtr</span><span class="p">,</span> <span class="n">hipIpcMemHandle_t</span> <span class="n">handle</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">flags</span><span class="p">);</span>
</pre></td> </tr></tbody></table></code></pre></figure> <ul> <li> <strong>close</strong>:</li> </ul> <p>After remote GPU used the remote memory, it calls <strong>hipIpcCloseMemHandle</strong> to delete the remote gpu address space from its own address space:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre><span class="c1">//close the imported remote memory handle.</span>
<span class="n">hipError_t</span> <span class="nf">hipIpcCloseMemHandle</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span> <span class="n">devPtr</span><span class="p">);</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p><img src="/assets/img/Scaling-Import-Export.png" alt="Scaling Import and Export " width="70%"></p> <p>During these import/export and the following remote memory process, these MMUs are involved: Three MMUs are used during GPU to GPU communication:</p> <ul> <li> <strong>GPU MMU</strong>: setup the TLB table during import, and translate translate virtual address to NPA.</li> <li> <strong>Port MMU</strong>: A table in Ethernet controller, or just a software implementation which maps GPU-ID to network MAC address.</li> <li> <strong>R-MMU</strong>:Target GPU MMU translate NPA to local SPA, also do accessible control and checks.</li> </ul> <p><strong>Addressing mapping in Shared Programming Model</strong></p> <p>In shared memory programming model, the MMU setup and translation process can be described as Following:</p> <p><img src="/assets/img/Scaling-Shared-Mem.png" alt="Scaling-Up-Shared-Mem" width="100%"></p> <ol> <li> <p><strong>shmem_init()</strong>: The Shared Memory library will build up a <strong>segment table</strong> first, which will record the <strong>shared</strong> memory segments, including the start address and size. This table is <strong>shared</strong> between all PEs and will be referenced when accessed remotely.</p> </li> <li> <strong>shmem_malloc()</strong>: User programming applies memories in the Symmetric shared heap. <ul> <li>All PE will do the same <strong>malloc()</strong> action and the <strong>shmem_malloc()</strong> will only return after all PE completes its operation.</li> <li>shmem_malloc() returns <strong>Local Virtual Address</strong> and different PE returns <strong>different VA</strong>, this VA only valid in this PE.</li> <li>The library will <strong>register</strong> the allocated memory regions in *R-MMU<strong>, so remote access are allowed for this memory region, and **PIN</strong> the corresponding physical pages so OS won’t swap out the pages.</li> </ul> </li> <li> <strong>shmem_access()</strong>: User program access remote memory via <strong>Local Virtual Address</strong> returned by malloc() and destination PE ID: <ul> <li> <strong>Symmetric Offset</strong> are calculated using <strong>segment table</strong> and <strong>local VA</strong>.</li> <li> <strong>Remote VA</strong> are generated using <strong>segment table</strong> and <strong>Symmetric Offset</strong>.</li> <li> <strong>Network Packet</strong> are composed using <strong>remote VA</strong>, <strong>data</strong> and <strong>command</strong>.</li> <li> <strong>Remote PE</strong>: Remote PE parsed the network packet, extract the <strong>remote VA</strong> as its <strong>local VA</strong>, and do the accesses. <strong>local VA</strong> are then translated to physical address with <strong>R-MMU</strong>.</li> </ul> </li> </ol> <hr> <h3 id="reference">Reference</h3> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div id="allen2021depth" class="col-sm-10"> <div class="title">In-depth analyses of unified virtual memory system for GPU accelerated computing</div> <div class="author"> Tyler Allen, and Rong Ge </div> <div class="periodical"> <em>In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'ShawnLess/JoeyXie',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>